{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Configurations\n",
    "CLASS_NAMES = [\n",
    "    \"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\",\n",
    "    \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"\n",
    "]\n",
    "IMAGE_DIM = (224, 224)\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation \n",
    "def download_dataset():\n",
    "    # Download and extract dataset\n",
    "    !wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n",
    "    !unzip -q nature_12K.zip\n",
    "    !rm nature_12K.zip\n",
    "\n",
    "def create_data_loaders(train_dir, test_dir, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_DIM),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Load training data and split into train/validation\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    class_indices = {cls: [] for cls in range(len(CLASS_NAMES))}\n",
    "    \n",
    "    for idx, (_, label) in enumerate(train_dataset.samples):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    train_indices, val_indices = [], []\n",
    "    for indices in class_indices.values():\n",
    "        trn, val = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "        train_indices.extend(trn)\n",
    "        val_indices.extend(val)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        Subset(train_dataset, train_indices),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        Subset(train_dataset, val_indices),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Test dataset\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Setup\n",
    "def initialize_model():\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # Freeze base layers\n",
    "    \n",
    "    # Modify final layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, len(CLASS_NAMES))\n",
    "    return model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Utilities \n",
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, dataloader, criterion, mode=\"Validation\"):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / len(dataloader.dataset)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"{mode} Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "def main():\n",
    "    wandb.init(project=\"Deep_Learning_Assignment_2\")\n",
    "    download_dataset()\n",
    "    \n",
    "    # Initialize components\n",
    "    model = initialize_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.NAdam(model.parameters(), lr=1e-4, weight_decay=0.005)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_dir='inaturalist_12K/train',\n",
    "        test_dir='inaturalist_12K/val',\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "    \n",
    "    # Final evaluation\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, \"Test\")\n",
    "    wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_acc})\n",
    "    wandb.finish()\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
